
| distributed init (rank 0): env://
number of params: 86567656
0 None None
1 None 0
1 None 0
1 None 0
1 None 0
1 None 1
1 None 1
1 None 1
1 None 1
1 None 2
1 None 2
1 None 2
1 None 2
1 None 3
1 None 3
1 None 3
1 None 3
1 None 4
1 None 4
1 None 4
1 None 4
1 None 5
1 None 5
1 None 5
1 None 5
1 None 6
1 None 6
1 None 6
1 None 6
1 None 7
1 None 7
1 None 7
1 None 7
1 None 8
1 None 8
1 None 8
1 None 8
1 None 9
1 None 9
1 None 9
1 None 9
1 None 10
1 None 10
1 None 10
1 None 10
1 None 11
1 None 11
1 None 11
1 None 11
2 None None
Register layer index and kernel shape:
[ 0]    module.patch_embed.proj -- kernel_shape: torch.Size([768, 3, 16, 16])
[ 1]   module.blocks.0.attn.qkv -- kernel_shape: torch.Size([2304, 768])
[ 2]  module.blocks.0.attn.proj -- kernel_shape: torch.Size([768, 768])
[ 3]    module.blocks.0.mlp.fc1 -- kernel_shape: torch.Size([3072, 768])
[ 4]    module.blocks.0.mlp.fc2 -- kernel_shape: torch.Size([768, 3072])
[ 5]   module.blocks.1.attn.qkv -- kernel_shape: torch.Size([2304, 768])
[ 6]  module.blocks.1.attn.proj -- kernel_shape: torch.Size([768, 768])
[ 7]    module.blocks.1.mlp.fc1 -- kernel_shape: torch.Size([3072, 768])
[ 8]    module.blocks.1.mlp.fc2 -- kernel_shape: torch.Size([768, 3072])
[ 9]   module.blocks.2.attn.qkv -- kernel_shape: torch.Size([2304, 768])
[10]  module.blocks.2.attn.proj -- kernel_shape: torch.Size([768, 768])
[11]    module.blocks.2.mlp.fc1 -- kernel_shape: torch.Size([3072, 768])
[12]    module.blocks.2.mlp.fc2 -- kernel_shape: torch.Size([768, 3072])
[13]   module.blocks.3.attn.qkv -- kernel_shape: torch.Size([2304, 768])
[14]  module.blocks.3.attn.proj -- kernel_shape: torch.Size([768, 768])
[15]    module.blocks.3.mlp.fc1 -- kernel_shape: torch.Size([3072, 768])
[16]    module.blocks.3.mlp.fc2 -- kernel_shape: torch.Size([768, 3072])
[17]   module.blocks.4.attn.qkv -- kernel_shape: torch.Size([2304, 768])
[18]  module.blocks.4.attn.proj -- kernel_shape: torch.Size([768, 768])
[19]    module.blocks.4.mlp.fc1 -- kernel_shape: torch.Size([3072, 768])
[20]    module.blocks.4.mlp.fc2 -- kernel_shape: torch.Size([768, 3072])
[21]   module.blocks.5.attn.qkv -- kernel_shape: torch.Size([2304, 768])
[22]  module.blocks.5.attn.proj -- kernel_shape: torch.Size([768, 768])
[23]    module.blocks.5.mlp.fc1 -- kernel_shape: torch.Size([3072, 768])
[24]    module.blocks.5.mlp.fc2 -- kernel_shape: torch.Size([768, 3072])
[25]   module.blocks.6.attn.qkv -- kernel_shape: torch.Size([2304, 768])
[26]  module.blocks.6.attn.proj -- kernel_shape: torch.Size([768, 768])
[27]    module.blocks.6.mlp.fc1 -- kernel_shape: torch.Size([3072, 768])
[28]    module.blocks.6.mlp.fc2 -- kernel_shape: torch.Size([768, 3072])
[29]   module.blocks.7.attn.qkv -- kernel_shape: torch.Size([2304, 768])
[30]  module.blocks.7.attn.proj -- kernel_shape: torch.Size([768, 768])
[31]    module.blocks.7.mlp.fc1 -- kernel_shape: torch.Size([3072, 768])
[32]    module.blocks.7.mlp.fc2 -- kernel_shape: torch.Size([768, 3072])
[33]   module.blocks.8.attn.qkv -- kernel_shape: torch.Size([2304, 768])
[34]  module.blocks.8.attn.proj -- kernel_shape: torch.Size([768, 768])
[35]    module.blocks.8.mlp.fc1 -- kernel_shape: torch.Size([3072, 768])
[36]    module.blocks.8.mlp.fc2 -- kernel_shape: torch.Size([768, 3072])
[37]   module.blocks.9.attn.qkv -- kernel_shape: torch.Size([2304, 768])
[38]  module.blocks.9.attn.proj -- kernel_shape: torch.Size([768, 768])
[39]    module.blocks.9.mlp.fc1 -- kernel_shape: torch.Size([3072, 768])
[40]    module.blocks.9.mlp.fc2 -- kernel_shape: torch.Size([768, 3072])
[41]  module.blocks.10.attn.qkv -- kernel_shape: torch.Size([2304, 768])
[42] module.blocks.10.attn.proj -- kernel_shape: torch.Size([768, 768])
[43]   module.blocks.10.mlp.fc1 -- kernel_shape: torch.Size([3072, 768])
[44]   module.blocks.10.mlp.fc2 -- kernel_shape: torch.Size([768, 3072])
[45]  module.blocks.11.attn.qkv -- kernel_shape: torch.Size([2304, 768])
[46] module.blocks.11.attn.proj -- kernel_shape: torch.Size([768, 768])
[47]   module.blocks.11.mlp.fc1 -- kernel_shape: torch.Size([3072, 768])
[48]   module.blocks.11.mlp.fc2 -- kernel_shape: torch.Size([768, 3072])
[49]                module.head -- kernel_shape: torch.Size([1000, 768])
Test:  [  0/131]  eta: 0:12:07  loss: 6.9780 (6.9780)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 5.5557  data: 4.5891  max mem: 3730
Test:  [ 10/131]  eta: 0:01:29  loss: 7.0186 (7.0457)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0473)  time: 0.7384  data: 0.4537  max mem: 3730
Test:  [ 20/131]  eta: 0:00:56  loss: 7.0670 (7.0513)  acc1: 0.0000 (0.1364)  acc5: 0.0000 (0.3224)  time: 0.2609  data: 0.0436  max mem: 3730
Test:  [ 30/131]  eta: 0:00:43  loss: 7.0741 (7.0676)  acc1: 0.0000 (0.1512)  acc5: 0.0000 (0.5460)  time: 0.2623  data: 0.0441  max mem: 3730
Test:  [ 40/131]  eta: 0:00:35  loss: 7.0348 (7.0574)  acc1: 0.0000 (0.1524)  acc5: 0.2604 (0.5716)  time: 0.2663  data: 0.0480  max mem: 3730
Test:  [ 50/131]  eta: 0:00:29  loss: 7.0348 (7.0596)  acc1: 0.0000 (0.1225)  acc5: 0.0000 (0.4953)  time: 0.2666  data: 0.0470  max mem: 3730
Test:  [ 60/131]  eta: 0:00:25  loss: 7.0384 (7.0576)  acc1: 0.0000 (0.1067)  acc5: 0.0000 (0.4355)  time: 0.2915  data: 0.0718  max mem: 3730
Test:  [ 70/131]  eta: 0:00:21  loss: 7.0264 (7.0499)  acc1: 0.0000 (0.1027)  acc5: 0.2604 (0.6162)  time: 0.2977  data: 0.0787  max mem: 3730
Test:  [ 80/131]  eta: 0:00:17  loss: 7.0320 (7.0501)  acc1: 0.0000 (0.0932)  acc5: 0.2604 (0.6141)  time: 0.2694  data: 0.0486  max mem: 3730
Test:  [ 90/131]  eta: 0:00:13  loss: 7.0338 (7.0513)  acc1: 0.0000 (0.0830)  acc5: 0.0000 (0.5638)  time: 0.2645  data: 0.0434  max mem: 3730
Test:  [100/131]  eta: 0:00:10  loss: 7.0152 (7.0487)  acc1: 0.0000 (0.0748)  acc5: 0.0000 (0.5337)  time: 0.2662  data: 0.0470  max mem: 3730
Test:  [110/131]  eta: 0:00:06  loss: 7.0119 (7.0448)  acc1: 0.0000 (0.0727)  acc5: 0.0000 (0.5513)  time: 0.2716  data: 0.0504  max mem: 3730
Test:  [120/131]  eta: 0:00:03  loss: 7.0417 (7.0484)  acc1: 0.0000 (0.0753)  acc5: 0.0000 (0.5445)  time: 0.2715  data: 0.0496  max mem: 3730
Test:  [130/131]  eta: 0:00:00  loss: 7.1069 (7.0559)  acc1: 0.0000 (0.0900)  acc5: 0.0000 (0.5520)  time: 0.2620  data: 0.0487  max mem: 3730
Test: Total time: 0:00:40 (0.3114 s / it)
* Acc@1 0.090 Acc@5 0.552 loss 7.056
Start training for 300 epochs
Epoch: [0]  [   0/1251]  eta: 1:39:14  lr: 0.000001  loss: 7.0453 (7.0453)  time: 4.7599  data: 2.8924  max mem: 26538
Epoch: [0]  [  10/1251]  eta: 0:18:58  lr: 0.000001  loss: 7.0419 (7.0467)  time: 0.9175  data: 0.2632  max mem: 27204
Epoch: [0]  [  20/1251]  eta: 0:14:54  lr: 0.000001  loss: 7.0535 (7.0564)  time: 0.5252  data: 0.0002  max mem: 27204
Epoch: [0]  [  30/1251]  eta: 0:13:31  lr: 0.000001  loss: 7.0478 (7.0480)  time: 0.5251  data: 0.0002  max mem: 27205
Epoch: [0]  [  40/1251]  eta: 0:12:48  lr: 0.000001  loss: 7.0345 (7.0463)  time: 0.5370  data: 0.0002  max mem: 27205
Epoch: [0]  [  50/1251]  eta: 0:12:21  lr: 0.000001  loss: 7.0345 (7.0441)  time: 0.5454  data: 0.0002  max mem: 27205
Epoch: [0]  [  60/1251]  eta: 0:12:04  lr: 0.000001  loss: 7.0363 (7.0447)  time: 0.5539  data: 0.0002  max mem: 27205
Epoch: [0]  [  70/1251]  eta: 0:11:51  lr: 0.000001  loss: 7.0356 (7.0418)  time: 0.5625  data: 0.0002  max mem: 27205
Epoch: [0]  [  80/1251]  eta: 0:11:41  lr: 0.000001  loss: 7.0329 (7.0408)  time: 0.5727  data: 0.0002  max mem: 27205
Epoch: [0]  [  90/1251]  eta: 0:11:33  lr: 0.000001  loss: 7.0168 (7.0382)  time: 0.5804  data: 0.0002  max mem: 27205
Epoch: [0]  [ 100/1251]  eta: 0:11:25  lr: 0.000001  loss: 7.0086 (7.0349)  time: 0.5816  data: 0.0002  max mem: 27205
Epoch: [0]  [ 110/1251]  eta: 0:11:17  lr: 0.000001  loss: 7.0086 (7.0329)  time: 0.5788  data: 0.0002  max mem: 27205
Epoch: [0]  [ 120/1251]  eta: 0:12:07  lr: 0.000001  loss: 7.0051 (7.0305)  time: 0.8845  data: 0.0002  max mem: 27205
Epoch: [0]  [ 130/1251]  eta: 0:12:04  lr: 0.000001  loss: 6.9993 (7.0278)  time: 0.9391  data: 0.0002  max mem: 27205
Traceback (most recent call last):
  File "main.py", line 862, in <module>
    if __name__ == '__main__':
  File "main.py", line 684, in main
    # finetune(model, data_loader_train, data_loader_val, sampler_train, criterion, pruner, n_parameters, device, args, dataset_val, loss_scaler, model_ema, mixup_fn, output_dir)
  File "main.py", line 789, in train
  File "/home/mingyuan/lab/deit/engine.py", line 67, in train_one_epoch
    loss_scaler(loss, optimizer, clip_grad=max_norm,
  File "/home/mingyuan/anaconda3/envs/deit/lib/python3.8/site-packages/timm/utils/cuda.py", line 46, in __call__
    self._scaler.step(optimizer)
  File "/home/mingyuan/anaconda3/envs/deit/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py", line 370, in step
    retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)
  File "/home/mingyuan/anaconda3/envs/deit/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py", line 289, in _maybe_opt_step
    if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):
  File "/home/mingyuan/anaconda3/envs/deit/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py", line 289, in <genexpr>
    if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):
KeyboardInterrupt