
| distributed init (rank 0): env://
number of params: 86567656
0 None None
1 None 0
1 None 0
1 None 0
1 None 0
1 None 1
1 None 1
1 None 1
1 None 1
1 None 2
1 None 2
1 None 2
1 None 2
1 None 3
1 None 3
1 None 3
1 None 3
1 None 4
1 None 4
1 None 4
1 None 4
1 None 5
1 None 5
1 None 5
1 None 5
1 None 6
1 None 6
1 None 6
1 None 6
1 None 7
1 None 7
1 None 7
1 None 7
1 None 8
1 None 8
1 None 8
1 None 8
1 None 9
1 None 9
1 None 9
1 None 9
1 None 10
1 None 10
1 None 10
1 None 10
1 None 11
1 None 11
1 None 11
1 None 11
2 None None
Register layer index and kernel shape:
[ 0]    module.patch_embed.proj -- kernel_shape: torch.Size([768, 3, 16, 16])
[ 1]   module.blocks.0.attn.qkv -- kernel_shape: torch.Size([2304, 768])
[ 2]  module.blocks.0.attn.proj -- kernel_shape: torch.Size([768, 768])
[ 3]    module.blocks.0.mlp.fc1 -- kernel_shape: torch.Size([3072, 768])
[ 4]    module.blocks.0.mlp.fc2 -- kernel_shape: torch.Size([768, 3072])
[ 5]   module.blocks.1.attn.qkv -- kernel_shape: torch.Size([2304, 768])
[ 6]  module.blocks.1.attn.proj -- kernel_shape: torch.Size([768, 768])
[ 7]    module.blocks.1.mlp.fc1 -- kernel_shape: torch.Size([3072, 768])
[ 8]    module.blocks.1.mlp.fc2 -- kernel_shape: torch.Size([768, 3072])
[ 9]   module.blocks.2.attn.qkv -- kernel_shape: torch.Size([2304, 768])
[10]  module.blocks.2.attn.proj -- kernel_shape: torch.Size([768, 768])
[11]    module.blocks.2.mlp.fc1 -- kernel_shape: torch.Size([3072, 768])
[12]    module.blocks.2.mlp.fc2 -- kernel_shape: torch.Size([768, 3072])
[13]   module.blocks.3.attn.qkv -- kernel_shape: torch.Size([2304, 768])
[14]  module.blocks.3.attn.proj -- kernel_shape: torch.Size([768, 768])
[15]    module.blocks.3.mlp.fc1 -- kernel_shape: torch.Size([3072, 768])
[16]    module.blocks.3.mlp.fc2 -- kernel_shape: torch.Size([768, 3072])
[17]   module.blocks.4.attn.qkv -- kernel_shape: torch.Size([2304, 768])
[18]  module.blocks.4.attn.proj -- kernel_shape: torch.Size([768, 768])
[19]    module.blocks.4.mlp.fc1 -- kernel_shape: torch.Size([3072, 768])
[20]    module.blocks.4.mlp.fc2 -- kernel_shape: torch.Size([768, 3072])
[21]   module.blocks.5.attn.qkv -- kernel_shape: torch.Size([2304, 768])
[22]  module.blocks.5.attn.proj -- kernel_shape: torch.Size([768, 768])
[23]    module.blocks.5.mlp.fc1 -- kernel_shape: torch.Size([3072, 768])
[24]    module.blocks.5.mlp.fc2 -- kernel_shape: torch.Size([768, 3072])
[25]   module.blocks.6.attn.qkv -- kernel_shape: torch.Size([2304, 768])
[26]  module.blocks.6.attn.proj -- kernel_shape: torch.Size([768, 768])
[27]    module.blocks.6.mlp.fc1 -- kernel_shape: torch.Size([3072, 768])
[28]    module.blocks.6.mlp.fc2 -- kernel_shape: torch.Size([768, 3072])
[29]   module.blocks.7.attn.qkv -- kernel_shape: torch.Size([2304, 768])
[30]  module.blocks.7.attn.proj -- kernel_shape: torch.Size([768, 768])
[31]    module.blocks.7.mlp.fc1 -- kernel_shape: torch.Size([3072, 768])
[32]    module.blocks.7.mlp.fc2 -- kernel_shape: torch.Size([768, 3072])
[33]   module.blocks.8.attn.qkv -- kernel_shape: torch.Size([2304, 768])
[34]  module.blocks.8.attn.proj -- kernel_shape: torch.Size([768, 768])
[35]    module.blocks.8.mlp.fc1 -- kernel_shape: torch.Size([3072, 768])
[36]    module.blocks.8.mlp.fc2 -- kernel_shape: torch.Size([768, 3072])
[37]   module.blocks.9.attn.qkv -- kernel_shape: torch.Size([2304, 768])
[38]  module.blocks.9.attn.proj -- kernel_shape: torch.Size([768, 768])
[39]    module.blocks.9.mlp.fc1 -- kernel_shape: torch.Size([3072, 768])
[40]    module.blocks.9.mlp.fc2 -- kernel_shape: torch.Size([768, 3072])
[41]  module.blocks.10.attn.qkv -- kernel_shape: torch.Size([2304, 768])
[42] module.blocks.10.attn.proj -- kernel_shape: torch.Size([768, 768])
[43]   module.blocks.10.mlp.fc1 -- kernel_shape: torch.Size([3072, 768])
[44]   module.blocks.10.mlp.fc2 -- kernel_shape: torch.Size([768, 3072])
[45]  module.blocks.11.attn.qkv -- kernel_shape: torch.Size([2304, 768])
[46] module.blocks.11.attn.proj -- kernel_shape: torch.Size([768, 768])
[47]   module.blocks.11.mlp.fc1 -- kernel_shape: torch.Size([3072, 768])
[48]   module.blocks.11.mlp.fc2 -- kernel_shape: torch.Size([768, 3072])
[49]                module.head -- kernel_shape: torch.Size([1000, 768])
Test:  [  0/131]  eta: 0:12:53  loss: 6.9945 (6.9945)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 5.9055  data: 5.1664  max mem: 4096
Test:  [ 10/131]  eta: 0:01:36  loss: 6.9657 (6.9663)  acc1: 0.0000 (0.0237)  acc5: 0.2604 (0.2841)  time: 0.7961  data: 0.5249  max mem: 4097
Test:  [ 20/131]  eta: 0:01:00  loss: 7.0116 (7.0262)  acc1: 0.0000 (0.1736)  acc5: 0.0000 (0.5332)  time: 0.2759  data: 0.0494  max mem: 4097
Test:  [ 30/131]  eta: 0:00:45  loss: 7.0989 (7.0563)  acc1: 0.0000 (0.1344)  acc5: 0.0000 (0.4704)  time: 0.2639  data: 0.0372  max mem: 4097
Test:  [ 40/131]  eta: 0:00:37  loss: 7.0979 (7.0561)  acc1: 0.0000 (0.1016)  acc5: 0.0000 (0.4129)  time: 0.2676  data: 0.0438  max mem: 4097
Test:  [ 50/131]  eta: 0:00:31  loss: 7.0540 (7.0621)  acc1: 0.0000 (0.0868)  acc5: 0.0000 (0.3779)  time: 0.2772  data: 0.0524  max mem: 4097
Test:  [ 60/131]  eta: 0:00:26  loss: 7.0540 (7.0626)  acc1: 0.0000 (0.0854)  acc5: 0.0000 (0.4312)  time: 0.2788  data: 0.0522  max mem: 4097
Test:  [ 70/131]  eta: 0:00:21  loss: 7.0679 (7.0654)  acc1: 0.0000 (0.0990)  acc5: 0.0000 (0.4328)  time: 0.2713  data: 0.0457  max mem: 4097
Test:  [ 80/131]  eta: 0:00:17  loss: 7.0695 (7.0644)  acc1: 0.0000 (0.0997)  acc5: 0.0000 (0.5015)  time: 0.2684  data: 0.0423  max mem: 4097
Test:  [ 90/131]  eta: 0:00:13  loss: 7.0400 (7.0620)  acc1: 0.0000 (0.0916)  acc5: 0.0000 (0.4722)  time: 0.2765  data: 0.0505  max mem: 4097
Test:  [100/131]  eta: 0:00:10  loss: 7.0291 (7.0603)  acc1: 0.0000 (0.0877)  acc5: 0.0000 (0.5208)  time: 0.2742  data: 0.0472  max mem: 4097
Test:  [110/131]  eta: 0:00:06  loss: 7.0232 (7.0524)  acc1: 0.0000 (0.0915)  acc5: 0.0000 (0.5678)  time: 0.2714  data: 0.0426  max mem: 4097
Test:  [120/131]  eta: 0:00:03  loss: 6.9689 (7.0457)  acc1: 0.0000 (0.1205)  acc5: 0.7812 (0.6392)  time: 0.2761  data: 0.0469  max mem: 4097
Test:  [130/131]  eta: 0:00:00  loss: 6.9944 (7.0446)  acc1: 0.0000 (0.1540)  acc5: 0.2604 (0.6700)  time: 0.2665  data: 0.0449  max mem: 4097
Test: Total time: 0:00:41 (0.3175 s / it)
* Acc@1 0.154 Acc@5 0.670 loss 7.045
Traceback (most recent call last):
  File "main.py", line 863, in <module>
    main(args)
  File "main.py", line 628, in main
    model = pruner.prune() # get the pruned model
  File "/home/mingyuan/lab/deit/pruner/reg_pruner.py", line 365, in prune
    loss_scaler(loss, self.optimizer, clip_grad=self.args.clip_grad,
  File "/home/mingyuan/lab/deit/pruner/reg_pruner.py", line 29, in __call__
    self._scaler.step(optimizer)
  File "/home/mingyuan/anaconda3/envs/deit/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py", line 370, in step
    retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)
  File "/home/mingyuan/anaconda3/envs/deit/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py", line 289, in _maybe_opt_step
    if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):
  File "/home/mingyuan/anaconda3/envs/deit/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py", line 289, in <genexpr>
    if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):
KeyboardInterrupt