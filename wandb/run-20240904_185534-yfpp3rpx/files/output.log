
| distributed init (rank 0): env://
number of params: 86567656
0 None None
1 None 0
1 None 0
1 None 0
1 None 0
1 None 1
1 None 1
1 None 1
1 None 1
1 None 2
1 None 2
1 None 2
1 None 2
1 None 3
1 None 3
1 None 3
1 None 3
1 None 4
1 None 4
1 None 4
1 None 4
1 None 5
1 None 5
1 None 5
1 None 5
1 None 6
1 None 6
1 None 6
1 None 6
1 None 7
1 None 7
1 None 7
1 None 7
1 None 8
1 None 8
1 None 8
1 None 8
1 None 9
1 None 9
1 None 9
1 None 9
1 None 10
1 None 10
1 None 10
1 None 10
1 None 11
1 None 11
1 None 11
1 None 11
2 None None
Register layer index and kernel shape:
[ 0]    module.patch_embed.proj -- kernel_shape: torch.Size([768, 3, 16, 16])
[ 1]   module.blocks.0.attn.qkv -- kernel_shape: torch.Size([2304, 768])
[ 2]  module.blocks.0.attn.proj -- kernel_shape: torch.Size([768, 768])
[ 3]    module.blocks.0.mlp.fc1 -- kernel_shape: torch.Size([3072, 768])
[ 4]    module.blocks.0.mlp.fc2 -- kernel_shape: torch.Size([768, 3072])
[ 5]   module.blocks.1.attn.qkv -- kernel_shape: torch.Size([2304, 768])
[ 6]  module.blocks.1.attn.proj -- kernel_shape: torch.Size([768, 768])
[ 7]    module.blocks.1.mlp.fc1 -- kernel_shape: torch.Size([3072, 768])
[ 8]    module.blocks.1.mlp.fc2 -- kernel_shape: torch.Size([768, 3072])
[ 9]   module.blocks.2.attn.qkv -- kernel_shape: torch.Size([2304, 768])
[10]  module.blocks.2.attn.proj -- kernel_shape: torch.Size([768, 768])
[11]    module.blocks.2.mlp.fc1 -- kernel_shape: torch.Size([3072, 768])
[12]    module.blocks.2.mlp.fc2 -- kernel_shape: torch.Size([768, 3072])
[13]   module.blocks.3.attn.qkv -- kernel_shape: torch.Size([2304, 768])
[14]  module.blocks.3.attn.proj -- kernel_shape: torch.Size([768, 768])
[15]    module.blocks.3.mlp.fc1 -- kernel_shape: torch.Size([3072, 768])
[16]    module.blocks.3.mlp.fc2 -- kernel_shape: torch.Size([768, 3072])
[17]   module.blocks.4.attn.qkv -- kernel_shape: torch.Size([2304, 768])
[18]  module.blocks.4.attn.proj -- kernel_shape: torch.Size([768, 768])
[19]    module.blocks.4.mlp.fc1 -- kernel_shape: torch.Size([3072, 768])
[20]    module.blocks.4.mlp.fc2 -- kernel_shape: torch.Size([768, 3072])
[21]   module.blocks.5.attn.qkv -- kernel_shape: torch.Size([2304, 768])
[22]  module.blocks.5.attn.proj -- kernel_shape: torch.Size([768, 768])
[23]    module.blocks.5.mlp.fc1 -- kernel_shape: torch.Size([3072, 768])
[24]    module.blocks.5.mlp.fc2 -- kernel_shape: torch.Size([768, 3072])
[25]   module.blocks.6.attn.qkv -- kernel_shape: torch.Size([2304, 768])
[26]  module.blocks.6.attn.proj -- kernel_shape: torch.Size([768, 768])
[27]    module.blocks.6.mlp.fc1 -- kernel_shape: torch.Size([3072, 768])
[28]    module.blocks.6.mlp.fc2 -- kernel_shape: torch.Size([768, 3072])
[29]   module.blocks.7.attn.qkv -- kernel_shape: torch.Size([2304, 768])
[30]  module.blocks.7.attn.proj -- kernel_shape: torch.Size([768, 768])
[31]    module.blocks.7.mlp.fc1 -- kernel_shape: torch.Size([3072, 768])
[32]    module.blocks.7.mlp.fc2 -- kernel_shape: torch.Size([768, 3072])
[33]   module.blocks.8.attn.qkv -- kernel_shape: torch.Size([2304, 768])
[34]  module.blocks.8.attn.proj -- kernel_shape: torch.Size([768, 768])
[35]    module.blocks.8.mlp.fc1 -- kernel_shape: torch.Size([3072, 768])
[36]    module.blocks.8.mlp.fc2 -- kernel_shape: torch.Size([768, 3072])
[37]   module.blocks.9.attn.qkv -- kernel_shape: torch.Size([2304, 768])
[38]  module.blocks.9.attn.proj -- kernel_shape: torch.Size([768, 768])
[39]    module.blocks.9.mlp.fc1 -- kernel_shape: torch.Size([3072, 768])
[40]    module.blocks.9.mlp.fc2 -- kernel_shape: torch.Size([768, 3072])
[41]  module.blocks.10.attn.qkv -- kernel_shape: torch.Size([2304, 768])
[42] module.blocks.10.attn.proj -- kernel_shape: torch.Size([768, 768])
[43]   module.blocks.10.mlp.fc1 -- kernel_shape: torch.Size([3072, 768])
[44]   module.blocks.10.mlp.fc2 -- kernel_shape: torch.Size([768, 3072])
[45]  module.blocks.11.attn.qkv -- kernel_shape: torch.Size([2304, 768])
[46] module.blocks.11.attn.proj -- kernel_shape: torch.Size([768, 768])
[47]   module.blocks.11.mlp.fc1 -- kernel_shape: torch.Size([3072, 768])
[48]   module.blocks.11.mlp.fc2 -- kernel_shape: torch.Size([768, 3072])
[49]                module.head -- kernel_shape: torch.Size([1000, 768])
Test:  [  0/131]  eta: 0:14:55  loss: 7.1424 (7.1424)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 6.8386  data: 5.9910  max mem: 3731
Test:  [ 10/131]  eta: 0:01:42  loss: 7.0304 (7.0433)  acc1: 0.0000 (0.1657)  acc5: 0.0000 (0.4735)  time: 0.8454  data: 0.5761  max mem: 3731
Test:  [ 20/131]  eta: 0:01:00  loss: 7.1061 (7.0766)  acc1: 0.0000 (0.1116)  acc5: 0.0000 (0.4216)  time: 0.2303  data: 0.0177  max mem: 3731
Test:  [ 30/131]  eta: 0:00:45  loss: 7.0534 (7.0636)  acc1: 0.0000 (0.0840)  acc5: 0.0000 (0.4032)  time: 0.2368  data: 0.0235  max mem: 3731
Test:  [ 40/131]  eta: 0:00:36  loss: 7.0363 (7.0631)  acc1: 0.0000 (0.0889)  acc5: 0.0000 (0.4319)  time: 0.2583  data: 0.0454  max mem: 3731
Test:  [ 50/131]  eta: 0:00:31  loss: 7.0310 (7.0522)  acc1: 0.0000 (0.0919)  acc5: 0.0000 (0.4545)  time: 0.2760  data: 0.0630  max mem: 3731
Test:  [ 60/131]  eta: 0:00:25  loss: 6.9695 (7.0339)  acc1: 0.0000 (0.0811)  acc5: 0.0000 (0.4781)  time: 0.2818  data: 0.0674  max mem: 3731
Test:  [ 70/131]  eta: 0:00:21  loss: 6.9836 (7.0404)  acc1: 0.0000 (0.0880)  acc5: 0.0000 (0.4548)  time: 0.2558  data: 0.0399  max mem: 3731
Test:  [ 80/131]  eta: 0:00:17  loss: 7.0458 (7.0428)  acc1: 0.0000 (0.0900)  acc5: 0.0000 (0.4630)  time: 0.2546  data: 0.0395  max mem: 3731
Test:  [ 90/131]  eta: 0:00:13  loss: 7.0533 (7.0456)  acc1: 0.0000 (0.0944)  acc5: 0.2604 (0.4922)  time: 0.2711  data: 0.0573  max mem: 3731
Test:  [100/131]  eta: 0:00:10  loss: 7.1066 (7.0524)  acc1: 0.0000 (0.0851)  acc5: 0.0000 (0.4667)  time: 0.2688  data: 0.0547  max mem: 3731
Test:  [110/131]  eta: 0:00:06  loss: 7.0708 (7.0498)  acc1: 0.0000 (0.0798)  acc5: 0.0000 (0.4716)  time: 0.2637  data: 0.0489  max mem: 3731
Test:  [120/131]  eta: 0:00:03  loss: 7.0325 (7.0505)  acc1: 0.0000 (0.0732)  acc5: 0.0000 (0.4412)  time: 0.2659  data: 0.0512  max mem: 3731
Test:  [130/131]  eta: 0:00:00  loss: 6.9857 (7.0497)  acc1: 0.0000 (0.0680)  acc5: 0.0000 (0.4460)  time: 0.2560  data: 0.0482  max mem: 3731
Test: Total time: 0:00:40 (0.3105 s / it)
* Acc@1 0.068 Acc@5 0.446 loss 7.050
Start training for 300 epochs
Epoch: [0]  [   0/1251]  eta: 1:41:39  lr: 0.000001  loss: 7.0265 (7.0265)  time: 4.8753  data: 3.0569  max mem: 26539
Epoch: [0]  [  10/1251]  eta: 0:19:45  lr: 0.000001  loss: 7.0431 (7.0452)  time: 0.9552  data: 0.2781  max mem: 27208
Epoch: [0]  [  20/1251]  eta: 0:15:14  lr: 0.000001  loss: 7.0431 (7.0448)  time: 0.5367  data: 0.0002  max mem: 27208
Epoch: [0]  [  30/1251]  eta: 0:13:38  lr: 0.000001  loss: 7.0473 (7.0458)  time: 0.5133  data: 0.0002  max mem: 27209
Epoch: [0]  [  40/1251]  eta: 0:12:47  lr: 0.000001  loss: 7.0340 (7.0412)  time: 0.5196  data: 0.0002  max mem: 27209
Epoch: [0]  [  50/1251]  eta: 0:12:17  lr: 0.000001  loss: 7.0276 (7.0381)  time: 0.5271  data: 0.0002  max mem: 27210
Epoch: [0]  [  60/1251]  eta: 0:11:56  lr: 0.000001  loss: 7.0228 (7.0351)  time: 0.5352  data: 0.0002  max mem: 27210
Epoch: [0]  [  70/1251]  eta: 0:11:41  lr: 0.000001  loss: 7.0305 (7.0344)  time: 0.5432  data: 0.0002  max mem: 27210
Epoch: [0]  [  80/1251]  eta: 0:11:31  lr: 0.000001  loss: 7.0168 (7.0315)  time: 0.5550  data: 0.0002  max mem: 27210
Epoch: [0]  [  90/1251]  eta: 0:11:22  lr: 0.000001  loss: 7.0095 (7.0294)  time: 0.5649  data: 0.0002  max mem: 27210
Epoch: [0]  [ 100/1251]  eta: 0:11:15  lr: 0.000001  loss: 7.0209 (7.0287)  time: 0.5744  data: 0.0002  max mem: 27210
Epoch: [0]  [ 110/1251]  eta: 0:11:09  lr: 0.000001  loss: 7.0146 (7.0270)  time: 0.5820  data: 0.0002  max mem: 27210
Epoch: [0]  [ 120/1251]  eta: 0:11:02  lr: 0.000001  loss: 7.0052 (7.0252)  time: 0.5765  data: 0.0002  max mem: 27210
Epoch: [0]  [ 130/1251]  eta: 0:10:54  lr: 0.000001  loss: 7.0052 (7.0244)  time: 0.5693  data: 0.0002  max mem: 27210
Epoch: [0]  [ 140/1251]  eta: 0:10:47  lr: 0.000001  loss: 7.0176 (7.0240)  time: 0.5647  data: 0.0002  max mem: 27210
Epoch: [0]  [ 150/1251]  eta: 0:10:39  lr: 0.000001  loss: 6.9930 (7.0218)  time: 0.5618  data: 0.0002  max mem: 27210
Epoch: [0]  [ 160/1251]  eta: 0:10:42  lr: 0.000001  loss: 6.9897 (7.0204)  time: 0.6317  data: 0.0002  max mem: 27210
Epoch: [0]  [ 170/1251]  eta: 0:10:43  lr: 0.000001  loss: 7.0039 (7.0196)  time: 0.7018  data: 0.0002  max mem: 27210
Epoch: [0]  [ 180/1251]  eta: 0:10:41  lr: 0.000001  loss: 6.9909 (7.0183)  time: 0.6843  data: 0.0002  max mem: 27210
Epoch: [0]  [ 190/1251]  eta: 0:10:34  lr: 0.000001  loss: 6.9854 (7.0167)  time: 0.6197  data: 0.0002  max mem: 27210
Epoch: [0]  [ 200/1251]  eta: 0:10:25  lr: 0.000001  loss: 6.9883 (7.0151)  time: 0.5559  data: 0.0002  max mem: 27211
Epoch: [0]  [ 210/1251]  eta: 0:10:15  lr: 0.000001  loss: 6.9941 (7.0140)  time: 0.5308  data: 0.0002  max mem: 27211
Epoch: [0]  [ 220/1251]  eta: 0:10:07  lr: 0.000001  loss: 6.9843 (7.0124)  time: 0.5342  data: 0.0002  max mem: 27211
Epoch: [0]  [ 230/1251]  eta: 0:09:58  lr: 0.000001  loss: 6.9967 (7.0120)  time: 0.5334  data: 0.0002  max mem: 27211
Epoch: [0]  [ 240/1251]  eta: 0:09:50  lr: 0.000001  loss: 6.9967 (7.0109)  time: 0.5215  data: 0.0002  max mem: 27211
Epoch: [0]  [ 250/1251]  eta: 0:09:41  lr: 0.000001  loss: 6.9832 (7.0097)  time: 0.5218  data: 0.0002  max mem: 27211
Epoch: [0]  [ 260/1251]  eta: 0:09:33  lr: 0.000001  loss: 6.9759 (7.0084)  time: 0.5217  data: 0.0002  max mem: 27211
Epoch: [0]  [ 270/1251]  eta: 0:09:26  lr: 0.000001  loss: 6.9711 (7.0071)  time: 0.5282  data: 0.0002  max mem: 27211
Epoch: [0]  [ 280/1251]  eta: 0:09:18  lr: 0.000001  loss: 6.9652 (7.0058)  time: 0.5279  data: 0.0003  max mem: 27211
Epoch: [0]  [ 290/1251]  eta: 0:09:11  lr: 0.000001  loss: 6.9722 (7.0051)  time: 0.5214  data: 0.0002  max mem: 27211
Epoch: [0]  [ 300/1251]  eta: 0:09:03  lr: 0.000001  loss: 6.9677 (7.0037)  time: 0.5225  data: 0.0002  max mem: 27211
Epoch: [0]  [ 310/1251]  eta: 0:08:56  lr: 0.000001  loss: 6.9657 (7.0023)  time: 0.5233  data: 0.0002  max mem: 27211
Epoch: [0]  [ 320/1251]  eta: 0:08:49  lr: 0.000001  loss: 6.9696 (7.0013)  time: 0.5239  data: 0.0002  max mem: 27211
Epoch: [0]  [ 330/1251]  eta: 0:08:42  lr: 0.000001  loss: 6.9743 (7.0008)  time: 0.5241  data: 0.0002  max mem: 27211
Epoch: [0]  [ 340/1251]  eta: 0:08:35  lr: 0.000001  loss: 6.9805 (6.9999)  time: 0.5241  data: 0.0002  max mem: 27211
Epoch: [0]  [ 350/1251]  eta: 0:08:29  lr: 0.000001  loss: 6.9589 (6.9989)  time: 0.5248  data: 0.0002  max mem: 27211
Epoch: [0]  [ 360/1251]  eta: 0:08:22  lr: 0.000001  loss: 6.9633 (6.9983)  time: 0.5252  data: 0.0002  max mem: 27211
Epoch: [0]  [ 370/1251]  eta: 0:08:15  lr: 0.000001  loss: 6.9763 (6.9977)  time: 0.5252  data: 0.0003  max mem: 27211
Epoch: [0]  [ 380/1251]  eta: 0:08:09  lr: 0.000001  loss: 6.9707 (6.9970)  time: 0.5259  data: 0.0003  max mem: 27211
Epoch: [0]  [ 390/1251]  eta: 0:08:03  lr: 0.000001  loss: 6.9647 (6.9961)  time: 0.5268  data: 0.0002  max mem: 27211
Epoch: [0]  [ 400/1251]  eta: 0:07:56  lr: 0.000001  loss: 6.9647 (6.9954)  time: 0.5323  data: 0.0002  max mem: 27211
Epoch: [0]  [ 410/1251]  eta: 0:07:50  lr: 0.000001  loss: 6.9654 (6.9947)  time: 0.5330  data: 0.0002  max mem: 27211
Epoch: [0]  [ 420/1251]  eta: 0:07:44  lr: 0.000001  loss: 6.9589 (6.9938)  time: 0.5278  data: 0.0002  max mem: 27211
Epoch: [0]  [ 430/1251]  eta: 0:07:38  lr: 0.000001  loss: 6.9497 (6.9929)  time: 0.5374  data: 0.0002  max mem: 27211
Epoch: [0]  [ 440/1251]  eta: 0:07:32  lr: 0.000001  loss: 6.9443 (6.9919)  time: 0.5420  data: 0.0002  max mem: 27211
Epoch: [0]  [ 450/1251]  eta: 0:07:26  lr: 0.000001  loss: 6.9469 (6.9912)  time: 0.5362  data: 0.0002  max mem: 27211
Epoch: [0]  [ 460/1251]  eta: 0:07:20  lr: 0.000001  loss: 6.9529 (6.9905)  time: 0.5320  data: 0.0002  max mem: 27211
Epoch: [0]  [ 470/1251]  eta: 0:07:14  lr: 0.000001  loss: 6.9533 (6.9898)  time: 0.5279  data: 0.0002  max mem: 27211
Epoch: [0]  [ 480/1251]  eta: 0:07:08  lr: 0.000001  loss: 6.9538 (6.9892)  time: 0.5281  data: 0.0002  max mem: 27211
Epoch: [0]  [ 490/1251]  eta: 0:07:02  lr: 0.000001  loss: 6.9518 (6.9884)  time: 0.5278  data: 0.0002  max mem: 27211
Epoch: [0]  [ 500/1251]  eta: 0:06:56  lr: 0.000001  loss: 6.9414 (6.9876)  time: 0.5273  data: 0.0002  max mem: 27211
Epoch: [0]  [ 510/1251]  eta: 0:06:50  lr: 0.000001  loss: 6.9414 (6.9867)  time: 0.5273  data: 0.0002  max mem: 27211
Epoch: [0]  [ 520/1251]  eta: 0:06:44  lr: 0.000001  loss: 6.9396 (6.9858)  time: 0.5266  data: 0.0002  max mem: 27211
Epoch: [0]  [ 530/1251]  eta: 0:06:38  lr: 0.000001  loss: 6.9453 (6.9852)  time: 0.5267  data: 0.0002  max mem: 27211
Epoch: [0]  [ 540/1251]  eta: 0:06:32  lr: 0.000001  loss: 6.9581 (6.9846)  time: 0.5273  data: 0.0002  max mem: 27211
Epoch: [0]  [ 550/1251]  eta: 0:06:27  lr: 0.000001  loss: 6.9499 (6.9839)  time: 0.5271  data: 0.0002  max mem: 27211
Epoch: [0]  [ 560/1251]  eta: 0:06:21  lr: 0.000001  loss: 6.9403 (6.9832)  time: 0.5273  data: 0.0002  max mem: 27211
Epoch: [0]  [ 570/1251]  eta: 0:06:15  lr: 0.000001  loss: 6.9341 (6.9824)  time: 0.5274  data: 0.0002  max mem: 27211
Epoch: [0]  [ 580/1251]  eta: 0:06:09  lr: 0.000001  loss: 6.9341 (6.9817)  time: 0.5269  data: 0.0002  max mem: 27211
Epoch: [0]  [ 590/1251]  eta: 0:06:03  lr: 0.000001  loss: 6.9483 (6.9810)  time: 0.5270  data: 0.0002  max mem: 27211
Epoch: [0]  [ 600/1251]  eta: 0:05:58  lr: 0.000001  loss: 6.9507 (6.9806)  time: 0.5271  data: 0.0002  max mem: 27211
Epoch: [0]  [ 610/1251]  eta: 0:05:52  lr: 0.000001  loss: 6.9485 (6.9800)  time: 0.5270  data: 0.0002  max mem: 27211
Epoch: [0]  [ 620/1251]  eta: 0:05:46  lr: 0.000001  loss: 6.9434 (6.9794)  time: 0.5323  data: 0.0002  max mem: 27211
Epoch: [0]  [ 630/1251]  eta: 0:05:41  lr: 0.000001  loss: 6.9367 (6.9788)  time: 0.5328  data: 0.0002  max mem: 27211
Epoch: [0]  [ 640/1251]  eta: 0:05:35  lr: 0.000001  loss: 6.9367 (6.9783)  time: 0.5282  data: 0.0002  max mem: 27211
Epoch: [0]  [ 650/1251]  eta: 0:05:29  lr: 0.000001  loss: 6.9503 (6.9777)  time: 0.5280  data: 0.0002  max mem: 27211
Epoch: [0]  [ 660/1251]  eta: 0:05:23  lr: 0.000001  loss: 6.9421 (6.9772)  time: 0.5279  data: 0.0002  max mem: 27211
Epoch: [0]  [ 670/1251]  eta: 0:05:18  lr: 0.000001  loss: 6.9421 (6.9766)  time: 0.5280  data: 0.0002  max mem: 27211
Epoch: [0]  [ 680/1251]  eta: 0:05:12  lr: 0.000001  loss: 6.9413 (6.9761)  time: 0.5275  data: 0.0002  max mem: 27211
Epoch: [0]  [ 690/1251]  eta: 0:05:07  lr: 0.000001  loss: 6.9359 (6.9755)  time: 0.5275  data: 0.0002  max mem: 27211
Epoch: [0]  [ 700/1251]  eta: 0:05:01  lr: 0.000001  loss: 6.9387 (6.9750)  time: 0.5283  data: 0.0002  max mem: 27211
Epoch: [0]  [ 710/1251]  eta: 0:04:55  lr: 0.000001  loss: 6.9380 (6.9743)  time: 0.5295  data: 0.0002  max mem: 27211
Epoch: [0]  [ 720/1251]  eta: 0:04:50  lr: 0.000001  loss: 6.9348 (6.9738)  time: 0.5298  data: 0.0002  max mem: 27211
Epoch: [0]  [ 730/1251]  eta: 0:04:44  lr: 0.000001  loss: 6.9345 (6.9732)  time: 0.5297  data: 0.0002  max mem: 27211
Epoch: [0]  [ 740/1251]  eta: 0:04:39  lr: 0.000001  loss: 6.9345 (6.9727)  time: 0.5300  data: 0.0002  max mem: 27211
Epoch: [0]  [ 750/1251]  eta: 0:04:33  lr: 0.000001  loss: 6.9404 (6.9723)  time: 0.5302  data: 0.0003  max mem: 27211
Epoch: [0]  [ 760/1251]  eta: 0:04:27  lr: 0.000001  loss: 6.9301 (6.9718)  time: 0.5310  data: 0.0003  max mem: 27211
Epoch: [0]  [ 770/1251]  eta: 0:04:22  lr: 0.000001  loss: 6.9301 (6.9713)  time: 0.5315  data: 0.0003  max mem: 27211
Epoch: [0]  [ 780/1251]  eta: 0:04:16  lr: 0.000001  loss: 6.9287 (6.9707)  time: 0.5316  data: 0.0002  max mem: 27211
Epoch: [0]  [ 790/1251]  eta: 0:04:11  lr: 0.000001  loss: 6.9287 (6.9702)  time: 0.5321  data: 0.0002  max mem: 27211
Epoch: [0]  [ 800/1251]  eta: 0:04:05  lr: 0.000001  loss: 6.9322 (6.9697)  time: 0.5318  data: 0.0002  max mem: 27211
Epoch: [0]  [ 810/1251]  eta: 0:04:00  lr: 0.000001  loss: 6.9322 (6.9693)  time: 0.5311  data: 0.0002  max mem: 27211
Epoch: [0]  [ 820/1251]  eta: 0:03:54  lr: 0.000001  loss: 6.9307 (6.9688)  time: 0.5313  data: 0.0002  max mem: 27211
Epoch: [0]  [ 830/1251]  eta: 0:03:49  lr: 0.000001  loss: 6.9249 (6.9683)  time: 0.5313  data: 0.0002  max mem: 27211
Epoch: [0]  [ 840/1251]  eta: 0:03:43  lr: 0.000001  loss: 6.9249 (6.9678)  time: 0.5309  data: 0.0002  max mem: 27211
Epoch: [0]  [ 850/1251]  eta: 0:03:38  lr: 0.000001  loss: 6.9190 (6.9672)  time: 0.5380  data: 0.0002  max mem: 27211
Epoch: [0]  [ 860/1251]  eta: 0:03:32  lr: 0.000001  loss: 6.9218 (6.9667)  time: 0.5383  data: 0.0002  max mem: 27211
Epoch: [0]  [ 870/1251]  eta: 0:03:27  lr: 0.000001  loss: 6.9209 (6.9661)  time: 0.5310  data: 0.0002  max mem: 27211
Epoch: [0]  [ 880/1251]  eta: 0:03:21  lr: 0.000001  loss: 6.9146 (6.9657)  time: 0.5307  data: 0.0002  max mem: 27211
Epoch: [0]  [ 890/1251]  eta: 0:03:16  lr: 0.000001  loss: 6.9305 (6.9652)  time: 0.5305  data: 0.0002  max mem: 27211
Epoch: [0]  [ 900/1251]  eta: 0:03:10  lr: 0.000001  loss: 6.9283 (6.9648)  time: 0.5305  data: 0.0002  max mem: 27211
Epoch: [0]  [ 910/1251]  eta: 0:03:05  lr: 0.000001  loss: 6.9238 (6.9643)  time: 0.5300  data: 0.0002  max mem: 27211
Epoch: [0]  [ 920/1251]  eta: 0:02:59  lr: 0.000001  loss: 6.9198 (6.9639)  time: 0.5288  data: 0.0002  max mem: 27211
Epoch: [0]  [ 930/1251]  eta: 0:02:54  lr: 0.000001  loss: 6.9266 (6.9635)  time: 0.5288  data: 0.0002  max mem: 27211
Epoch: [0]  [ 940/1251]  eta: 0:02:48  lr: 0.000001  loss: 6.9273 (6.9632)  time: 0.5290  data: 0.0002  max mem: 27211
Epoch: [0]  [ 950/1251]  eta: 0:02:43  lr: 0.000001  loss: 6.9204 (6.9627)  time: 0.5289  data: 0.0002  max mem: 27211
Epoch: [0]  [ 960/1251]  eta: 0:02:38  lr: 0.000001  loss: 6.9177 (6.9623)  time: 0.5924  data: 0.0002  max mem: 27211
Epoch: [0]  [ 970/1251]  eta: 0:02:32  lr: 0.000001  loss: 6.9226 (6.9618)  time: 0.5903  data: 0.0002  max mem: 27211
Traceback (most recent call last):
  File "main.py", line 863, in <module>
  File "main.py", line 684, in main
    train(model, criterion, optimizer, lr_scheduler, n_parameters, device, data_loader_train, data_loader_val, dataset_val, loss_scaler, model_ema, mixup_fn, output_dir)
  File "main.py", line 789, in train
    data_loader_train.sampler.set_epoch(epoch)
  File "/home/mingyuan/lab/deit/engine.py", line 67, in train_one_epoch
    loss_scaler(loss, optimizer, clip_grad=max_norm,
  File "/home/mingyuan/anaconda3/envs/deit/lib/python3.8/site-packages/timm/utils/cuda.py", line 46, in __call__
    self._scaler.step(optimizer)
  File "/home/mingyuan/anaconda3/envs/deit/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py", line 370, in step
    retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)
  File "/home/mingyuan/anaconda3/envs/deit/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py", line 289, in _maybe_opt_step
    if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):
  File "/home/mingyuan/anaconda3/envs/deit/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py", line 289, in <genexpr>
    if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):
KeyboardInterrupt